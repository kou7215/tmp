{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "d-sne.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGALxqm9J826",
        "outputId": "9d189285-4033-495c-896d-aa6b4c7f6df9"
      },
      "source": [
        "!pip install tensorflow_addons"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_addons in /usr/local/lib/python3.7/dist-packages (0.13.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons) (2.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HalACnSlbN1G"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import (layers, models, optimizers, Model,\n",
        "                              activations, datasets, regularizers)\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_addons as tfa\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.io import loadmat\n",
        "%matplotlib inline\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5yrbxXjUxhd"
      },
      "source": [
        "# params\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 128\n",
        "LR_MAX = 1e-3\n",
        "LR_MIN = 1e-5\n",
        "WARMUP_EPOCH = 10\n",
        "START_ANNEALING_EPOCH = 10\n",
        "END_ANNEALING_EPOCH = 100"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRvdy_H1fFga"
      },
      "source": [
        "workspace_dir = 'drive/MyDrive/Colab Notebooks/d-SNE'\n",
        "if os.getcwd() != workspace_dir:\n",
        "  os.chdir(workspace_dir)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVjtjNRCcEyx"
      },
      "source": [
        "## load datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dsp3RooNcBog",
        "outputId": "637ccaca-a89b-49a3-8f27-506949c4434a"
      },
      "source": [
        "# load\n",
        "(x_train_mnist, y_train_mnist),(x_test_mnist, y_test_mnist) = datasets.mnist.load_data()\n",
        "\n",
        "x_train_mnist = [cv2.resize(x, (32,32), interpolation=cv2.INTER_AREA) for x in x_train_mnist]\n",
        "x_test_mnist = [cv2.resize(x, (32,32), interpolation=cv2.INTER_AREA) for x in x_test_mnist]\n",
        "# to 3ch\n",
        "x_train_mnist = np.array(x_train_mnist)[...,np.newaxis]\n",
        "x_test_mnist = np.array(x_test_mnist)[...,np.newaxis]\n",
        "x_train_mnist = np.array([np.concatenate((x,x,x), axis=2) for x in x_train_mnist])\n",
        "x_test_mnist = np.array([np.concatenate((x,x,x), axis=2) for x in x_test_mnist])\n",
        "\n",
        "# to one-hot\n",
        "y_train_mnist = np_utils.to_categorical(y_train_mnist)\n",
        "y_test_mnist = np_utils.to_categorical(y_test_mnist)\n",
        "\n",
        "# split\n",
        "x_train_mnist, x_valid_mnist, y_train_mnist, y_valid_mnist = train_test_split(x_train_mnist, y_train_mnist, test_size=0.1)\n",
        "x_train_mnist.shape, x_valid_mnist.shape, y_train_mnist.shape, y_valid_mnist.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((54000, 32, 32, 3), (6000, 32, 32, 3), (54000, 10), (6000, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3ohkHDpcRkX",
        "outputId": "c9fcd750-937b-4c8d-bd33-b3d9174581e6"
      },
      "source": [
        "# load\n",
        "train_raw = loadmat('datasets/train_32x32.mat')\n",
        "test_raw = loadmat('datasets/test_32x32.mat')\n",
        "x_train_svhn, y_train_svhn = train_raw['X'], train_raw['y']\n",
        "x_test_svhn, y_test_svhn = test_raw['X'], test_raw['y']\n",
        "\n",
        "# samplesize first\n",
        "x_train_svhn = np.transpose(x_train_svhn, (3,0,1,2))\n",
        "x_test_svhn = np.transpose(x_test_svhn, (3,0,1,2))\n",
        "                                         \n",
        "                                         \n",
        "# label 10 to 0\n",
        "y_train_svhn = np.squeeze(y_train_svhn)\n",
        "y_test_svhn = np.squeeze(y_test_svhn)\n",
        "y_train_svhn[y_train_svhn == 10] = 0\n",
        "y_test_svhn[y_test_svhn == 10] = 0\n",
        "\n",
        "# to onehot\n",
        "y_train_svhn = np_utils.to_categorical(y_train_svhn, 10)\n",
        "y_test_svhn = np_utils.to_categorical(y_test_svhn, 10)\n",
        "\n",
        "# split\n",
        "x_train_svhn, x_valid_svhn, y_train_svhn, y_valid_svhn = train_test_split(x_train_svhn, y_train_svhn, test_size=0.1)\n",
        "x_train_svhn.shape, x_valid_svhn.shape, y_train_svhn.shape, y_valid_svhn.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((65931, 32, 32, 3), (7326, 32, 32, 3), (65931, 10), (7326, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpLsO60il0De",
        "outputId": "f9f7fd06-49a5-43e1-9d2a-10e753f24448"
      },
      "source": [
        "x_train_svhn_10samples = []\n",
        "y_train_svhn_10samples = []\n",
        "for l in range(10):\n",
        "  cnt_per_class = 0\n",
        "  for i in range(len(x_train_svhn)):\n",
        "    label = np.argmax(y_train_svhn[i])\n",
        "    if label == l:\n",
        "      x_train_svhn_10samples.append(x_train_svhn[i])\n",
        "      y_train_svhn_10samples.append(y_train_svhn[i])\n",
        "      cnt_per_class += 1\n",
        "      if cnt_per_class >= 10:\n",
        "        break\n",
        "\n",
        "x_train_svhn_10samples = np.asarray(x_train_svhn_10samples)\n",
        "y_train_svhn_10samples = np.asarray(y_train_svhn_10samples)\n",
        "x_train_svhn_10samples.shape, y_train_svhn_10samples.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((100, 32, 32, 3), (100, 10))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMlmMwwrgR7_"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "def preprocess_train(x, y):\n",
        "  x = x / 255\n",
        "  x = tf.image.random_brightness(x, 0.2)\n",
        "  x = tf.image.random_contrast(x, 0.8, 1.2)\n",
        "  x = tf.image.random_crop(x, [BATCH_SIZE, 28, 28, 3])\n",
        "  x = tf.image.resize(x, [32, 32], tf.image.ResizeMethod.BICUBIC)\n",
        "  return x, y\n",
        "\n",
        "def preprocess_test(x, y):\n",
        "  x = x / 255\n",
        "  return x, y\n",
        "\n",
        "def create_train_datasets(x, y, batch_size=BATCH_SIZE):\n",
        "  ds = tf.data.Dataset.from_tensor_slices((x,y))\n",
        "  ds = ds.shuffle(len(x))\n",
        "  ds = ds.batch(batch_size, drop_remainder=True)\n",
        "  ds = ds.map(preprocess_train, num_parallel_calls=AUTOTUNE)\n",
        "  return ds\n",
        "\n",
        "def create_test_datasets(x, y, batch_size=BATCH_SIZE):\n",
        "  ds = tf.data.Dataset.from_tensor_slices((x,y))\n",
        "  ds = ds.batch(batch_size, drop_remainder=True)\n",
        "  ds = ds.map(preprocess_test, num_parallel_calls=AUTOTUNE)\n",
        "  return ds\n",
        "\n",
        "train_mnist_ds = create_train_datasets(x_train_mnist, y_train_mnist)\n",
        "train_svhn_ds = create_train_datasets(x_train_svhn_10samples, y_train_svhn_10samples)\n",
        "valid_mnist_ds = create_test_datasets(x_valid_mnist, y_valid_mnist)\n",
        "valid_svhn_ds = create_test_datasets(x_valid_svhn, y_valid_svhn)\n",
        "test_mnist_ds = create_test_datasets(x_test_mnist, y_test_mnist)\n",
        "test_svhn_ds = create_test_datasets(x_test_svhn, y_test_svhn)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2lbpP1URL8v"
      },
      "source": [
        "for x_t, y_t in train_svhn_ds:\n",
        "  print(x_t.shape, y_t.shape)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbIMgmfYHnsG"
      },
      "source": [
        "def _make_conv_block(block_index, num_chan=32, num_layer=2, stride=1, pad=2):\n",
        "  convs = []\n",
        "  for _ in range(num_layer):\n",
        "    convs.append(layers.Conv2D(num_chan, kernel_size=(3,3), strides=stride, padding='same'))\n",
        "    convs.append(layers.LeakyReLU(0.2))\n",
        "  convs.append(layers.MaxPool2D((2,2)))\n",
        "  return convs\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCGatqlOI7Rf"
      },
      "source": [
        "class AngularLinear(layers.Layer):\n",
        "  def __init__(self, classes):\n",
        "    super(AngularLinear, self).__init__()\n",
        "    self.classes = classes\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.kernel = self.add_variable(\"w_angular\",\n",
        "                                    shape=[int(input_shape[-1]), classes])\n",
        "\n",
        "  def call(self, x):\n",
        "    x_normed = K.l2_normalize(x, axis=-1)\n",
        "    kernel_normed = K.l2_normalize(self.kernel, axis=-1)\n",
        "    cos_theta = tf.matmul(x_normed, kernel_normed, name='cos_theta')\n",
        "    cos_theta = K.clip(cos_theta, -1, 1)\n",
        "    return cos_theta\n",
        "\n",
        "class LeNetPlus(Model):\n",
        "  \"\"\"\n",
        "  LeNetPlus model\n",
        "  \"\"\"\n",
        "  def __init__(self, classes=10, feature_size=256, use_dropout=True, use_norm=False, use_bn=False, use_inn=False,\n",
        "               use_angular=False, **kwargs):\n",
        "    super(LeNetPlus, self).__init__()\n",
        "    self.num_chans = [32, 64, 128]\n",
        "    self.use_dropout = use_dropout\n",
        "    self.use_norm = use_norm\n",
        "    self.feature_size = feature_size\n",
        "    self.use_bn = use_bn\n",
        "    self.use_inn = use_inn\n",
        "    self.use_angular = use_angular\n",
        "\n",
        "    #self.features = gluon.nn.HybridSequential(prefix='')\n",
        "    self.features = []\n",
        "\n",
        "    if self.use_inn:\n",
        "        #self.features.add(gluon.nn.InstanceNorm())\n",
        "        self.features.append(tfa.layers.InstanceNormalization())\n",
        "    for i, num_chan in enumerate(self.num_chans):\n",
        "        if use_bn:\n",
        "            self.features.append(layers.BatchNormalization())\n",
        "\n",
        "        self.features += _make_conv_block(i, num_chan=num_chan)\n",
        "\n",
        "        if use_dropout and i > 0:\n",
        "            self.features.append(layers.Dropout(0.5))\n",
        "\n",
        "    self.features.append(layers.Flatten())\n",
        "    self.features.append(layers.Dense(self.feature_size))\n",
        "\n",
        "    if self.use_norm:\n",
        "      #self.features.add(L2Normalization(mode='instance'))\n",
        "      self.features.append(layers.Lambda(lambda x : K.l2_normalize(x, axis=-1)))\n",
        "\n",
        "    if use_angular:\n",
        "      self.outputs = AngularLinear(classes)\n",
        "    else:\n",
        "      self.outputs = layers.Dense(classes)\n",
        "\n",
        "  def call(self, x):\n",
        "    for f in self.features:\n",
        "      x = f(x)\n",
        "    features = x\n",
        "    preds = self.outputs(features)\n",
        "    preds = activations.softmax(preds)\n",
        "    #print(outputs)\n",
        "    return preds, features\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr12vAnteQ2A"
      },
      "source": [
        "class dSNEloss(tf.keras.losses.Loss):\n",
        "  def __init__(self, margin=1.0, feature_norm=False, name='dSNEloss'):\n",
        "    super().__init__(name=name)\n",
        "    self.margin = margin\n",
        "    self.feature_norm = feature_norm\n",
        "\n",
        "  def __call__(self, feature_s, y_s, feature_t, y_t):\n",
        "    if self.feature_norm:\n",
        "      feature_s = layers.Lambda(lambda x : K.l2_normalize(x, axis=-1))(feature_s)\n",
        "      feature_t = layers.Lambda(lambda x : K.l2_normalize(x, axis=-1))(feature_t)\n",
        "    bs_s, h_s, w_s, ch_s = BATCH_SIZE, 32, 32, 512#tf.shape(feature_s)\n",
        "    bs_t, h_t, w_t, ch_t = BATCH_SIZE, 32, 32, 512#tf.shape(feature_t)\n",
        "    features_s_repeat = tf.broadcast_to(tf.expand_dims(feature_s, axis=0), shape=(bs_t, bs_s, ch_s))\n",
        "    features_t_repeat = tf.broadcast_to(tf.expand_dims(feature_t, axis=1), shape=(bs_t, bs_s, ch_s))\n",
        "\n",
        "    dists = tf.math.reduce_sum(tf.square(features_t_repeat - features_s_repeat), axis=2)\n",
        "\n",
        "    if len(tf.shape(y_t)) != 1:\n",
        "      y_t = tf.argmax(y_t, axis=-1)\n",
        "    if len(tf.shape(y_s)) != 1:\n",
        "      y_s = tf.argmax(y_s, axis=-1) \n",
        "    y_t_repeat = tf.broadcast_to(tf.expand_dims(y_t, axis=1), shape=(bs_t, bs_s))\n",
        "    y_s_repeat = tf.broadcast_to(tf.expand_dims(y_s, axis=0), shape=(bs_t, bs_s))\n",
        "    y_t_repeat = tf.cast(y_t_repeat, tf.int32)\n",
        "    y_s_repeat = tf.cast(y_s_repeat, tf.int32)\n",
        "\n",
        "    y_same = tf.equal(y_t_repeat, y_s_repeat)\n",
        "    y_diff = tf.not_equal(y_t_repeat, y_s_repeat)\n",
        "    y_same = tf.cast(y_same, tf.float32)\n",
        "    y_diff = tf.cast(y_diff, tf.float32)\n",
        "\n",
        "    intra_cls_dists = dists * y_same\n",
        "    inter_cls_dists = dists * y_diff\n",
        "\n",
        "    max_dists = tf.math.reduce_max(dists, axis=1, keepdims=True)\n",
        "    max_dists = tf.broadcast_to(max_dists, shape=(bs_t, bs_s))\n",
        "    revised_inter_cls_dists = tf.where(tf.cast(y_same, tf.bool), max_dists, inter_cls_dists)\n",
        "\n",
        "    max_intra_cls_dist = tf.math.reduce_max(intra_cls_dists, axis=1)\n",
        "    min_inter_cls_dist = tf.math.reduce_min(revised_inter_cls_dists, axis=1)\n",
        "\n",
        "    loss = tf.nn.relu(max_intra_cls_dist - min_inter_cls_dist + self.margin)\n",
        "    return loss    "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_Suaf7jdTvM",
        "outputId": "2b3064b1-0c18-44dc-8395-45479a2801a8"
      },
      "source": [
        "model = LeNetPlus(classes=10, feature_size=512, use_dropout=True, \n",
        "                  use_norm=False, use_bn=False, use_inn=True, \n",
        "                  use_angular=False)\n",
        "model.build((BATCH_SIZE,32,32,3))\n",
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"le_net_plus\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "instance_normalization (Inst multiple                  6         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              multiple                  896       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)      multiple                  0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            multiple                  9248      \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    multiple                  0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) multiple                  0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            multiple                  18496     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    multiple                  0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            multiple                  36928     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    multiple                  0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 multiple                  0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            multiple                  73856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    multiple                  0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            multiple                  147584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    multiple                  0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 multiple                  0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  1049088   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  5130      \n",
            "=================================================================\n",
            "Total params: 1,341,232\n",
            "Trainable params: 1,341,232\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLvXXKB4lTcL"
      },
      "source": [
        "# define model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plBpTxsmVQJY"
      },
      "source": [
        "def __warmup(epoch, warmup_epoch, warmup_rate, lr_end, visible=False):\n",
        "    warmup_grad = warmup_rate / warmup_epoch\n",
        "    if epoch <= warmup_epoch:\n",
        "        lr = lr_end * 10**(-warmup_rate + warmup_grad*epoch)\n",
        "    if visible:\n",
        "        print('lr : {}'.format(lr))\n",
        "        return lr\n",
        "    else:\n",
        "        return lr\n",
        "\n",
        "def __cosine_annealing(epoch, lr_max, lr_min, epoch_start, epoch_end, visible=False):\n",
        "    if epoch <= epoch_start:\n",
        "        # before annealing\n",
        "        return lr_max\n",
        "    elif epoch_start < epoch <= epoch_end:\n",
        "        # during annealing\n",
        "        lr = lr_min + 0.5*(lr_max - lr_min)*(1+np.cos(np.pi*(epoch-epoch_start)/(epoch_end - epoch_start)))\n",
        "        if visible:\n",
        "            print('lr : {}'.format(lr))\n",
        "        return lr\n",
        "    else:\n",
        "        # after annealing\n",
        "        return lr\n",
        "\n",
        "def update_lr(epoch):\n",
        "    visible = False\n",
        "    warmup_epoch = 10\n",
        "    if epoch <= warmup_epoch:\n",
        "        lr = __warmup(epoch, warmup_epoch=WARMUP_EPOCH, warmup_rate=2.0, lr_end=LR_MAX, visible=visible)\n",
        "    else:\n",
        "        lr = __cosine_annealing(epoch,\n",
        "                                lr_max=LR_MAX,\n",
        "                                lr_min=LR_MIN,\n",
        "                                epoch_start=START_ANNEALING_EPOCH,\n",
        "                                epoch_end=END_ANNEALING_EPOCH,\n",
        "                                visible=visible)\n",
        "    return lr"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tamjEiuoV2ZO"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFDTGMz4eZBI"
      },
      "source": [
        "x_entropy_loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "dsne_loss = dSNEloss()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LR_MIN)\n",
        "\n",
        "log_train_loss_total = tf.keras.metrics.Mean(name='train_loss_total')\n",
        "log_train_loss_x_ent = tf.keras.metrics.Mean(name='train_x_ent_loss')\n",
        "log_train_loss_d_sne = tf.keras.metrics.Mean(name='train_d_sne_loss')\n",
        "\n",
        "log_valid_loss_total = tf.keras.metrics.Mean(name='valid_loss_total')\n",
        "log_valid_loss_x_ent = tf.keras.metrics.Mean(name='valid_x_ent_loss')\n",
        "log_valid_loss_d_sne = tf.keras.metrics.Mean(name='valid_d_sne_loss')\n",
        "\n",
        "log_train_accuracy_mnist = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy_mnist')\n",
        "log_train_accuracy_svhn = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy_svhn')\n",
        "\n",
        "log_valid_accuracy_mnist = tf.keras.metrics.CategoricalAccuracy(name='valid_accuracy_mnist')\n",
        "log_valid_accuracy_svhn = tf.keras.metrics.CategoricalAccuracy(name='valid_accuracy_svhn')\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouSvQpEfUD4I"
      },
      "source": [
        "@tf.function\n",
        "def train_step(x_s, y_s, x_t, y_t):\n",
        "  with tf.GradientTape() as tape:\n",
        "    preds_s, feature_s = model(x_s)\n",
        "    preds_t, feature_t = model(x_t)\n",
        "    train_x_entropy_s = x_entropy_loss(y_s, preds_s)\n",
        "    train_x_entropy_t = x_entropy_loss(y_t, preds_t)\n",
        "    train_x_entropy = train_x_entropy_s + train_x_entropy_t\n",
        "    train_dsne = dsne_loss(feature_s=feature_s, y_s=y_s, feature_t=feature_t, y_t=y_t)\n",
        "    #train_loss = 0.9*train_x_entropy + 0.1*train_dsne\n",
        "    #train_loss = [0.9*train_x_entropy, 0.1*train_dsne]\n",
        "  gradients = tape.gradient(train_loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  # records\n",
        "  log_train_loss_total(train_loss)\n",
        "  log_train_loss_x_ent(train_x_entropy)\n",
        "  log_train_loss_d_sne(train_dsne)\n",
        "\n",
        "  log_train_accuracy_mnist(y_s, preds_s)\n",
        "  log_train_accuracy_svhn(y_t, preds_t)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd-F-rC9CAyc"
      },
      "source": [
        "@tf.function\n",
        "def valid_step(x_s, y_s, x_t, y_t):\n",
        "  preds_s, feature_s = model(x_s)\n",
        "  preds_t, feature_t = model(x_t)\n",
        "  valid_x_entropy_s = x_entropy_loss(y_s, preds_s)\n",
        "  valid_x_entropy_t = x_entropy_loss(y_t, preds_t)\n",
        "  valid_x_entropy = valid_x_entropy_s + valid_x_entropy_t\n",
        "  valid_dsne = dsne_loss(feature_s=feature_s, y_s=y_s, feature_t=feature_t, y_t=y_t)\n",
        "  valid_loss = 0.9*valid_x_entropy + 0.1*valid_dsne\n",
        "\n",
        "  # records\n",
        "  log_valid_loss_total(valid_loss)\n",
        "  log_valid_loss_x_ent(valid_x_entropy)\n",
        "  log_valid_loss_d_sne(valid_dsne)\n",
        "\n",
        "  log_valid_accuracy_mnist(y_s, preds_s)\n",
        "  log_valid_accuracy_svhn(y_t, preds_t)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCyaPlxbpQ-U",
        "outputId": "0dff6271-a185-4a14-ddad-401a122bc5c1"
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  for x_s, y_s in train_mnist_ds:\n",
        "    for x_t, y_t in train_svhn_ds:\n",
        "      train_step(x_s, y_s, x_t, y_t)\n",
        "\n",
        "  for x_s, y_s in valid_mnist_ds:\n",
        "    for x_t, y_t in valid_svhn_ds:\n",
        "      valid_step(x_s, y_s, x_t, y_t)\n",
        "  \n",
        "  optimizer.lr = update_lr(epoch)\n",
        "  print(\"epoch : {}\".format(epoch))\n",
        "  print('train_loss : {}, valid_loss : {}'.format(log_train_loss_total.result(), log_valid_loss_total.result()))\n",
        "  print('train x ent : {}, valid x ent : {}'.format(log_train_loss_x_ent.result(), log_valid_loss_x_ent.result()))\n",
        "  print('train dsne : {}, valid dsne : {}'.format(log_train_loss_d_sne.result(), log_valid_loss_d_sne.result()))\n",
        "  print('train mnist acc : {}, valid mnist acc : {}'.format(log_train_accuracy_mnist.result(), log_valid_accuracy_mnist.result()))\n",
        "  print('train svhn acc : {}, valid svhn acc : {}'.format(log_train_accuracy_svhn.result(), log_valid_accuracy_svhn.result()))\n",
        "\n",
        "  # 次のエポック用にメトリクスをリセット\n",
        "  log_train_loss_total.reset_states()\n",
        "  log_valid_loss_total.reset_states()\n",
        "  log_train_loss_x_ent.reset_states()\n",
        "  log_valid_loss_x_ent.reset_states()\n",
        "  log_train_loss_d_sne.reset_states()\n",
        "  log_valid_loss_d_sne.reset_states()\n",
        "  log_train_accuracy_mnist.reset_states()\n",
        "  log_valid_accuracy_mnist.reset_states()\n",
        "  log_train_accuracy_svhn.reset_states()\n",
        "  log_valid_accuracy_svhn.reset_states()\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch : 0\n",
            "train_loss : 0.0, valid_loss : 4.3658342361450195\n",
            "train x ent : 0.0, valid x ent : 4.635626316070557\n",
            "train dsne : 0.0, valid dsne : 1.9376541376113892\n",
            "train mnist acc : 0.0, valid mnist acc : 0.10190217196941376\n",
            "train svhn acc : 0.0, valid svhn acc : 0.06661184132099152\n",
            "epoch : 1\n",
            "train_loss : 0.0, valid_loss : 4.3658342361450195\n",
            "train x ent : 0.0, valid x ent : 4.635626316070557\n",
            "train dsne : 0.0, valid dsne : 1.9376541376113892\n",
            "train mnist acc : 0.0, valid mnist acc : 0.10190217196941376\n",
            "train svhn acc : 0.0, valid svhn acc : 0.06661184132099152\n",
            "epoch : 2\n",
            "train_loss : 0.0, valid_loss : 4.3658342361450195\n",
            "train x ent : 0.0, valid x ent : 4.635626316070557\n",
            "train dsne : 0.0, valid dsne : 1.9376541376113892\n",
            "train mnist acc : 0.0, valid mnist acc : 0.10190217196941376\n",
            "train svhn acc : 0.0, valid svhn acc : 0.06661184132099152\n",
            "epoch : 3\n",
            "train_loss : 0.0, valid_loss : 4.3658342361450195\n",
            "train x ent : 0.0, valid x ent : 4.635626316070557\n",
            "train dsne : 0.0, valid dsne : 1.9376541376113892\n",
            "train mnist acc : 0.0, valid mnist acc : 0.10190217196941376\n",
            "train svhn acc : 0.0, valid svhn acc : 0.06661184132099152\n",
            "epoch : 4\n",
            "train_loss : 0.0, valid_loss : 4.3658342361450195\n",
            "train x ent : 0.0, valid x ent : 4.635626316070557\n",
            "train dsne : 0.0, valid dsne : 1.9376541376113892\n",
            "train mnist acc : 0.0, valid mnist acc : 0.10190217196941376\n",
            "train svhn acc : 0.0, valid svhn acc : 0.06661184132099152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spJ3939AFAWT"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}